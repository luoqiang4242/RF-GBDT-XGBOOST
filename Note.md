RF、GBDT、XGBoost（https://blog.csdn.net/qq_28031525/article/details/70207918）
####
1、集成学习包括：
>>>>1）boosting：不同的分类器是通过串行训练而获得，每个新分类器都根据已训练的分类器的性能来进行训练，结果是每棵树累加  
2）bagging：随机抽样样本，多数表决，并行生成（RF随机森林）

2、RF
#
步骤：
##
	1)随机选择样本，放回抽样
	2)随机选择特征
	3)构建决策树
理论：决策树学习：采用自顶向下的递归的方法，基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处熵值为0（叶节点中的实例都属于一类）（ID3算法使用信息增益、C4.5算法使用信息增益率、CART算法使用基尼系数。C4.5是ID3基础上解决ID3信息增益会偏向那些取值较多的特征)  (https://blog.csdn.net/weixin_36586536/article/details/80468426）

                		构建决策树分为：
                        	a.建树:
                                	分类树：决策树方法是会把每个特征都试一遍，然后选取那个，能够使分类分的最好的特征，也就是说将A属性作为父节点，产生的纯度增益（GainA）要大于B属性作为父节点，则A作为优先选取的属性。
                                	回归树：使用最小方差作为分裂规则（归树对输入空间的划分采用一种启发式的方法，会遍历所有输入变量，找到最优的切分变量j和最优的切分点s，即选择第j个特征xj和它的取值s将输入空间划分为两部分，然后重复这个操作）
                        	b.剪枝:
          		4)随机森林投票（平均）

	3、GBDT
        	原理：gbdt通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练（弱分类器一般选择CART 回归树）
        	在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x)ft−1(x), 损失函数是L(y,ft−1(x))L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)ht(x)，让本轮的损失损失L(y,ft(x)=L(y,ft−1(x)+ht(x))L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 
        常用的损失函数：

    
