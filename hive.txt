hive 练习：
准备工作：启动hdfs
---------------------data-------------------
create external table if not exists default.dept(
deptno int,
dname string,
loc int
)
row format delimited fields terminated by '\t';

create external table if not exists default.emp(
empno int,
ename string,
job string,
mgr int,
hiredate string, 
sal double, 
comm double,
deptno int)
row format delimited fields terminated by '\t';

10	ACCOUNTING	1700
20	RESEARCH	1800
30	SALES	1900
40	OPERATIONS	1700

7369	SMITH	CLERK	7902	1980-12-17	800.00		20
7499	ALLEN	SALESMAN	7698	1981-2-20	1600.00	300.00	30
7521	WARD	SALESMAN	7698	1981-2-22	1250.00	500.00	30
7566	JONES	MANAGER	7839	1981-4-2	2975.00		20
7654	MARTIN	SALESMAN	7698	1981-9-28	1250.00	1400.00	30
7698	BLAKE	MANAGER	7839	1981-5-1	2850.00		30
7782	CLARK	MANAGER	7839	1981-6-9	2450.00		10
7788	SCOTT	ANALYST	7566	1987-4-19	3000.00		20
7839	KING	PRESIDENT		1981-11-17	5000.00		10
7844	TURNER	SALESMAN	7698	1981-9-8	1500.00	0.00	30
7876	ADAMS	CLERK	7788	1987-5-23	1100.00		20
7900	JAMES	CLERK	7698	1981-12-3	950.00		30
7902	FORD	ANALYST	7566	1981-12-3	3000.00		20
7934	MILLER	CLERK	7782	1982-1-23	1300.00		10
------------------------------表--------------------------------------
创建表：
create table test(
name string,
friends array<string>,
children map<string,int>,
address struct<street:string, city:string>)
row format delimited fields terminated by ','
collection items terminated by '_'
map keys terminated by ':'
lines terminated by '\n';
数据
songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing
yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing

查看建表信息：
desc test;
desc formatted test;
show tables;

导入本地数据：
load data local inpath '/opt/module/test.txt' into table test;

查找数据：
select friends[1], children["xiao song"],address.city from test where name="songsong";
按列查询：
select ename AS name, deptno dn from emp; AS后面跟别名
算术运算符查询：
 select sal +1 from emp;
 常用函数查询：
 select count(*) cnt from emp;
 select max(sal) max_sal from emp;
 select min(sal) min_sal from emp;
 select sum(sal) sum_sal from emp; 
 select avg(sal) avg_sal from emp;
 
使用WHERE 子句，将不满足条件的行过滤掉:
select * from emp where sal >1000;

分组查询 groupby：
 select num , avg(num) avg_num from test0 group by num;
（1）where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据。
 avg(num) avg_num from test0 group by num having avg_num >15;

清空表中数据：
truncate table test;

①外部表：比如某个公司的原始日志数据存放在一个目录中，多个部门对这些原始数据进行分析，那么创建外部表是明智选择，这样原始数据不会被删除；
create external table test(.....) ..;
②内部表：对原始数据或比较重要的中间数据进行建表存储；
create table test(.....) ....;
③分区表：将每个小时或每天的日志文件进行分区存储，可以针对某个特定时间段做业务分析，而不必分析扫描所有数据；
create table test(.... ) partitioned by (month string)...;
③桶表：
create table test(...) clustered by(id)   into 4 buckets ...；

修改表
ALTER TABLE table_name RENAME TO new_table_name；改名字
alter table dept_partition add columns(deptdesc string);添加列
alter table dept_partition change column deptdesc desc int;更新列名
alter table dept_partition replace columns(deptno string, dname string, loc string);替换


数据导入：
hive>load data [local] inpath '/opt/module/datas/student.txt' [overwrite] into table student [partition (partcol1=val1,…)];
（1）load data:表示加载数据
（2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表
（3）inpath:表示加载数据的路径
（4）into table:表示加载到哪张表
（5）student:表示具体的表
（6）overwrite:表示覆盖表中已有数据，否则表示追加
（7）partition:表示上传到指定分区


数据导出：
导出到本地
hive (default)> insert overwrite local directory '/opt/module/datas/export/student' select * from student;
导出到HDFS  
hive (default)> insert overwrite directory '/user/atguigu/hive/warehouse/student2'
             ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' COLLECTION ITEMS TERMINATED BY '\n'
             select * from student;

			 
join：
内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。
hive (default)> select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;
左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。
hive (default)> select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno = d.deptno;
右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。
hive (default)> select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno;
满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。
hive (default)> select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno;


排序：
select * from emp order by sal;
select * from emp order by sal desc;//ASC（ascend）: 升序（默认）/DESC（descend）: 降序
select ename, sal*2 twosal from emp order by twosal;
二次排序：select ename, deptno, sal from emp order by deptno, sal ;

每个MapReduce内部排序（Sort By）
set mapreduce.job.reduces=3;
查看set mapreduce.job.reduces;
select * from emp sort by empno desc;//根据部门降序查看员工信息
insert overwrite local directory '/opt/module/datas/sortby-result' select * from emp sort by deptno desc;//将查询结果导入到文件中（按照部门编号降序排序）

分区排序（Distribute By）注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。
insert overwrite local directory '/opt/module/datas/distby-desc' select * from emp distribute by deptno sort by empno desc;//先按照部门编号分区，再按照员工编号降序排序。

cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒序排序，不能指定排序规则为ASC或者DESC。
select * from emp cluster by deptno;select * from emp distribute by deptno sort by deptno;两者等价

打开查询步骤执行情况基本语法
EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query
2）案例实操
（1）查看下面这条语句的执行计划
hive (default)> explain select * from emp;
hive (default)> explain select deptno, avg(sal) avg_sal from emp group by deptno;
（2）查看详细执行计划
hive (default)> explain extended select * from emp;
hive (default)> explain extended select deptno, avg(sal) avg_sal from emp group by deptno;

---------------------------数据库-------------------------------------------
创建数据库database：
create database db_hive;
查看数据库database：
show databases;
show databases like "db_hive*";
desc database extended db_hive;
默认数据库存放地址：
http://192.168.1.101:50070 查看在hdfs中的:/user/hive/warehouse
使用数据db_hive:
use db_hive;
删除数据库：
drop database db_hive cascade;
-------------------------function-------------------------------

hive 函数使用：
查看系统函数：show functions;
desc functions extended map;

自定义fun函数：
1.创建jar包;
三种方式: 一进一出 UDF;多进一出 UDAF;一进多出 UDTF;
package com.test.hive;
import org.apache.hadoop.hive.ql.exec.UDF;
public class Lower extends UDF {

	public String convert(final String s) {
		
		if (s == null) {
			return null;
		}
		
		return s.toString().toLowerCase();
	}
}
2.将jar包添加到hive的classpath
 add jar /opt/module/jars/udf.jar;
3.创建临时函数与开发好的java class关联
create temporary function my_lower as "com.test.hive.Lower";
4.在hql中使用自定义的函数strip 
select ename, my_lower(ename) from emp;

-----------------------------调优------------------------------
1）fetch抓取：
Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。
三种模式：
1、都走MR：
设置：set hive.fetch.task.conversion=none;
select * from emp;
2、执行查询语句，如下查询方式都不会执行mapreduce程序。
set hive.fetch.task.conversion=more;
select * from emp;

2)处理小数据集使用本地模式：
小表、大表Join:将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；
大表Join大表:
1、空KEY过滤
2、空key转换
有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上
select n.* from (select * from nullidtable where id is not null ) n  left join ori o on n.id = o.id;

3）如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。
设置自动选择Mapjoin
set hive.auto.convert.join = true; 默认为true
大表小表的阀值设置（默认25M一下认为是小表）：
set hive.mapjoin.smalltable.filesize=25000000;

4）group by数据倾斜
Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。
并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。
开启Map端聚合参数设置
是否在Map端进行聚合，默认为True
hive.map.aggr = true
在Map端进行聚合操作的条目数目
hive.groupby.mapaggr.checkinterval = 100000
有数据倾斜的时候进行负载均衡（默认是false）
hive.groupby.skewindata = true

5）Count(Distinct) 去重统计
数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换：
select count(id) from (select id from bigtable group by id) a;
 
尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积

6）行列过滤
列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。
先过滤再关联//select b.id from bigtable b join (select id from ori where id <= 10 ) o on b.id = o.id;
优于先关联在过滤：select o.id from bigtable b join ori o on o.id = b.id where o.id <= 10;

行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤

7)动态分区调整
关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。

8)数据倾斜
a.小文件合并：
在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。
set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

b.复杂文件增加Map数
当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。
增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。

c、调整reduce个数方法一
每个Reduce处理的数据量默认是256MB
hive.exec.reducers.bytes.per.reducer=256000000
每个任务最大的reduce数，默认为1009
hive.exec.reducers.max=1009
计算reducer数的公式
N=min(参数2，总输入数据量/参数1)

d、调整reduce个数方法二
在hadoop的mapred-default.xml文件中修改
设置每个job的Reduce个数
set mapreduce.job.reduces = 15;
reduce个数并不是越多越好
过多的启动和初始化reduce也会消耗时间和资源；
另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；
在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；


9)Hive提供了一个严格模式，可以防止用户执行那些可能意向不到的不好的影响的查询。
通过设置属性hive.mapred.mode值为默认是非严格模式nonstrict 。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询。
对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。
于使用了order by语句的查询，要求必须使用limit语句。
限制笛卡尔积的查询。

10)JVM重用
JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。
Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。

11）推测执行
在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。
设置开启推测执行参数：Hadoop的mapred-site.xml文件中进行配置


------------------面试-------------------
Hive本质：
将HQL语句转化为MR程序；
Hive处理的数据存储在HDFS中；
Hive分析数据底层的实现是Mapreduce
执行程序运行在yarn上

Hive架构概述

HIVE和数据库的区别

调优：mapjoin 分桶分区 数据倾斜  如何处理小文件   负责文件增加map个数  并行执行  JVM重用 表的优化  压缩
